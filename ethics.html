<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>IT Ethics - Fahad Nawaz</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f9f9f9;
        }
        nav {
            background-color: #333;
            padding: 10px;
        }
        .navbar {
            list-style-type: none;
            margin: 0;
            padding: 0;
        }
        .navbar li {
            display: inline;
            margin-right: 20px;
        }
        .navbar a {
            color: white;
            text-decoration: none;
            font-weight: bold;
        }
        h1, h2 {
            color: #333;
        }
        p, ul {
            max-width: 900px;
            line-height: 1.6;
        }
    </style>
</head>
<body>
    <nav>
        <ul class="navbar">
            <li><a href="index.html">Home</a></li>
            <li><a href="ethics.html">Ethics</a></li>
        </ul>
    </nav>

        <h1>Ethical Perspectives on Machine Learning</h1>

        <p>The way we look at machine learning—and tech like generative AI—can really depend on the ethical lens we use. Different ethical frameworks ask different questions: Is it helping the most people? Is it fair? Is it honest? Depending on how you answer those, the social impact of ML can be seen in totally different ways.</p>

        <h2>Utilitarian View</h2>
        <p>Utilitarianism generally works upon the concept of the greater good, whatever provides the biggest net benefit to society is considered the right thing to do regardless of the potential negative outcomes</p>
        <p>From a utilitarian point of view, machine learning is mostly seen in a positive light. It's all about the overall outcome—does this tech do more good than harm? And with ML, there’s a lot of good. It's fast, super efficient, and it opens up access to tools and knowledge for tons of people. Like, think about a student who's stuck on a homework problem. They can now just ask an AI tutor and get an answer instantly instead of having to wait for a teacher or book a tutor session. Or someone with no design skills can ask an AI to create a logo, or build them a website from scratch. Before, they might’ve had to pay someone a lot of money or just give up on the idea altogether. Even in healthcare, ML models are being used to spot diseases early or assist in surgeries. In farming, they’re helping predict crop failure before it happens. These are real, large-scale benefits.</p>
        <p>Yes, of course, there are downsides. People are worried about job loss, about AI replacing humans in creative or repetitive roles. There’s the issue of misinformation—AI can say the wrong thing very confidently. But from a utilitarian view, these might be seen as necessary sacrifices if the overal good is big enough. Still, most would agree that regulation is needed to make sure that harm doesn't go too far or hit certain groups unfairly.</p>

        <h2>Deontological View</h2>
        <p>Deontological Ethics focuses a lot more on what is the morally correct option, regardless of outcome, it makes decisions based off the rights, privacy, and the way the action is viewed by the individual.</p>
        <p>Now if we look at this from a deontological lens, the focus shifts. This perspective isn’t just about whether the outcome is good—it’s about whether the actions themselves are right or wrong. So even if ML helps a lot of people, it can still be seen as morally wrong if it violates someone’s rights.</p>
        <p>A common example is how generative AI is trained. It often uses tons of data from the internet—art, music, writing, photos—all pulled from real people, without their permission. That’s a big problem from this view. It doesn’t really matter if the final result is useful or even impressive. If it was built by taking work from artists or creators without asking, then it’s a violation of their rights. Think about a company that uses AI to make art for a movie. Instead of paying real artists, they just use generative tools that were trained on other people’s styles. That might save money, but it’s also cutting corners in a way that feels dishonest—even like digital plagiarism. From a deontological angle, that would be considered wrong, plain and simple. It’s not about whether the AI is "better" or faster. It’s about how it was made, and whether people were treated fairly.</p>
</body>
</html>
